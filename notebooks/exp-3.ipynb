{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd\u001b[39m(row):\n\u001b[1;32m     28\u001b[0m    \u001b[38;5;28;01mreturn\u001b[39;00m row[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39mrow[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39mrow[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m---> 30\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMFD_metadatas.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mShootdate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShootdate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39mnan, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     32\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdropna()\n",
      "File \u001b[0;32m~/.venvs/vtf/lib/python3.9/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/vtf/lib/python3.9/site-packages/pandas/io/parsers/readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/vtf/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1697\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m     (\n\u001b[1;32m   1701\u001b[0m         index,\n\u001b[1;32m   1702\u001b[0m         columns,\n\u001b[1;32m   1703\u001b[0m         col_dict,\n\u001b[0;32m-> 1704\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.venvs/vtf/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/.venvs/vtf/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:812\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.venvs/vtf/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:889\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.venvs/vtf/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:1047\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.venvs/vtf/lib/python3.9/site-packages/pandas/core/dtypes/common.py:1335\u001b[0m, in \u001b[0;36mis_extension_array_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;66;03m# Note: if other EA dtypes are ever held in HybridBlock, exclude those\u001b[39;00m\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;66;03m#  here too.\u001b[39;00m\n\u001b[1;32m   1328\u001b[0m     \u001b[38;5;66;03m# NB: need to check DatetimeTZDtype and not is_datetime64tz_dtype\u001b[39;00m\n\u001b[1;32m   1329\u001b[0m     \u001b[38;5;66;03m#  to exclude ArrowTimestampUSDtype\u001b[39;00m\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1331\u001b[0m         dtype, (DatetimeTZDtype, PeriodDtype)\n\u001b[1;32m   1332\u001b[0m     )\n\u001b[0;32m-> 1335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_extension_array_dtype\u001b[39m(arr_or_dtype) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;124;03m    Check if an object is a pandas extension array type.\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1380\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(arr_or_dtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, arr_or_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "CSV_ROWS = [\n",
    "    \"Photo_Name\",\n",
    "    \"Id\",\n",
    "    \"Sex\",\n",
    "    \"dob\",\n",
    "    \"dob_estimated\",\n",
    "    \"error_dob\",\n",
    "    \"FaceView\",\n",
    "    \"FaceQual\",\n",
    "    \"Shootdate\"\n",
    "]\n",
    "\n",
    "def csvdate_to_date(shoot_date):\n",
    "    year, month, day = shoot_date.split(\"-\")\n",
    "    return datetime.date(int(year), int(month), int(day))    \n",
    "\n",
    "def compute_age(row):\n",
    "    photo_date = csvdate_to_date(row[\"Shootdate\"])\n",
    "    dob_date = csvdate_to_date(row[\"dob\"])\n",
    "    age = photo_date - dob_date\n",
    "    return age.days\n",
    "\n",
    "def add(row):\n",
    "   return row[0]+row[1]+row[2]\n",
    "\n",
    "data = pd.read_csv(\"MFD_metadatas.csv\", dtype={'Shootdate': str})\n",
    "data['Shootdate'].replace('nan', np.nan, inplace=True)\n",
    "data = data.dropna()\n",
    "data['age'] = data.apply(compute_age, axis=1)\n",
    "\n",
    "def filter_by_age(data, age_in_days):\n",
    "    return data[data['age'] <= age_in_days]\n",
    "\n",
    "def filter_by_certainty(data):\n",
    "    return data[data['dob_estimated'] == False]\n",
    "\n",
    "def filter_dob_errors(data):\n",
    "    return data[data[\"age\"] >= 0]\n",
    "\n",
    "data[\"age\"].hist(bins=25)\n",
    "\n",
    "data = filter_by_certainty(data)\n",
    "data = filter_dob_errors(data)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "data[\"age\"].hist(bins=25)\n",
    "\n",
    "max_days = 365\n",
    "\n",
    "one_year_data = filter_by_age(data, age_in_days=max_days)\n",
    "one_year_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "hist = one_year_data[\"age\"].hist(bins=25)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.models import resnet18\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Define your custom dataset\n",
    "class MandrillImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, dataframe, img_size=(224, 224), device=\"cuda\", in_mem=True):\n",
    "        self.df = dataframe\n",
    "        self.root_dir = root_dir\n",
    "        self.img_size = img_size\n",
    "        self.in_mem = in_mem\n",
    "        if self.in_mem:\n",
    "            self.images = []\n",
    "            for i in tqdm(range(len(self.df))):\n",
    "                row = self.df.iloc[[i]]\n",
    "                self.images.append(self.load_photo(row))\n",
    "\n",
    "    def load_photo(self, row):\n",
    "        image_path = self.photo_path(row)\n",
    "        image = cv2.imread(image_path)\n",
    "        if image.shape[0:2] != self.img_size:\n",
    "            image = cv2.resize(image, self.img_size, interpolation = cv2.INTER_AREA)\n",
    "        image = np.moveaxis(image, -1, 0) # Channel first format\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        return image\n",
    "            \n",
    "    def photo_path(self, row):\n",
    "        return os.path.join(self.root_dir, f\"{row['Id'].values[0]}\", f\"{row['Photo_Name'].values[0]}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def _getpair(self, idx):\n",
    "        row = self.df.iloc[[idx]]\n",
    "\n",
    "        target = float(row[\"age\"].values[0])\n",
    "        target = target / 365\n",
    "\n",
    "        if self.in_mem:\n",
    "            image = self.images[idx]\n",
    "        else:\n",
    "            image = self.load_photo(row)\n",
    "        return torch.tensor(image).to(device), torch.tensor(target).to(device)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self._getpair(idx)\n",
    "\n",
    "class MandrillDualImageDataset(MandrillImageDataset):\n",
    "    def __init__(self, root_dir, dataframe, img_size=(224, 224), device=\"cuda\", in_mem=True):\n",
    "        super(MandrillDualImageDataset, self).__init__(root_dir, dataframe, img_size, device, in_mem)\n",
    "        self.max_year_gap = 25\n",
    "        self.max_days_gap = self.max_year_gap\n",
    "                    \n",
    "    def get_same_mandrill_idx(self, mandrill_id):\n",
    "        ids = self.df[self.df.Id == mandrill_id].index\n",
    "        return ids\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Randomize\n",
    "        idx = random.randint(0, len(self)-1)\n",
    "        #row = self.df.iloc[[idx]]\n",
    "        #mandrill_id = row[\"Id\"].values[0]\n",
    "        #ids = self.get_same_mandrill_idx(mandrill_id)\n",
    "        #second_idx = random.choice(ids)\n",
    "        \n",
    "        second_idx = random.randint(0, len(self)-1)\n",
    "        x1, y1 = self._getpair(idx)\n",
    "        x2, y2 = self._getpair(second_idx)\n",
    "\n",
    "        y1 = y1.cpu().numpy()\n",
    "        y2 = y2.cpu().numpy()\n",
    "        sign = np.sign(y1 - y2)\n",
    "        y = torch.zeros([3], device=device)\n",
    "        y[int(sign) + 1] = 1\n",
    "        return [x1, x2], y\n",
    "\n",
    "# Define your model\n",
    "from collections import OrderedDict\n",
    "def build_layers(n, method, prev_features, current_features, down=True):\n",
    "    layers = OrderedDict()\n",
    "    for i in range(n):\n",
    "        layers[str(i)] = method(prev_features, current_features)\n",
    "        prev_features = current_features\n",
    "        if down:\n",
    "            current_features = current_features // 2\n",
    "        else:\n",
    "            current_features = current_features * 2\n",
    "    return layers, prev_features\n",
    "\n",
    "    \n",
    "class BackboneRegressionModel(nn.Module):\n",
    "    def __init__(self, im_size=224, n_channels=3, conv_start=64, n_convs=5):\n",
    "        super(BackboneRegressionModel, self).__init__()\n",
    "        previous_feature_size = n_channels\n",
    "        current_feature_size = conv_start\n",
    "        convs_layers, last_feature_size = build_layers(\n",
    "            n_convs, self.conv_block, previous_feature_size, current_feature_size, down=False\n",
    "        )\n",
    "        self.conv_blocks = nn.Sequential(convs_layers)\n",
    "        \n",
    "        last_scale = (2 ** (n_convs))\n",
    "        last_size = im_size // last_scale\n",
    "        self.last_layer_size = last_feature_size * last_size * last_size\n",
    "\n",
    "    def conv_block(self, in_features, out_features):\n",
    "        conv = nn.Conv2d(in_features, out_features, 3, padding=\"same\")\n",
    "        relu = nn.ReLU(inplace=True)\n",
    "        bn = nn.BatchNorm2d(out_features)\n",
    "        pool = nn.MaxPool2d((2,2))\n",
    "        return nn.Sequential(conv, relu, bn, pool)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv_blocks(x)\n",
    "\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, cnn_backbone, lin_start=2048, n_lin=6):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.cnn_backbone = cnn_backbone\n",
    "\n",
    "        ############ Dense layers ########\n",
    "        previous_feature_size = self.cnn_backbone.last_layer_size\n",
    "        current_feature_size = lin_start\n",
    "        lin_layers, last_feature_size = build_layers(\n",
    "            n_lin, self.block, previous_feature_size, current_feature_size\n",
    "        )\n",
    "        self.blocks = nn.Sequential(lin_layers)\n",
    "        self.age = nn.Linear(last_feature_size, 1)\n",
    "\n",
    "    def block(self, in_features, out_features):\n",
    "        lin = nn.Linear(in_features, out_features)\n",
    "        relu = nn.ReLU(inplace=True)\n",
    "        return nn.Sequential(lin, relu)\n",
    "\n",
    "    def __del__(self):\n",
    "        del self.conv_blocks\n",
    "        del self.blocks\n",
    "        del self.x_coords\n",
    "        del self.y_coords\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.cnn_backbone(x)\n",
    "        z = torch.reshape(z, (z.shape[0], z.shape[1] * z.shape[2] * z.shape[3]))\n",
    "        z = self.blocks(z)\n",
    "        z = self.age(z)\n",
    "        z = torch.reshape(z, (z.shape[0],))\n",
    "        return z\n",
    "\n",
    "class DualRegressionModel(nn.Module):\n",
    "    def __init__(self, cnn_backbone, lin_start=2048, n_lin=6):\n",
    "        super(DualRegressionModel, self).__init__()\n",
    "        self.cnn_backbone = cnn_backbone\n",
    "        \n",
    "        previous_feature_size = 2* self.cnn_backbone.last_layer_size\n",
    "        current_feature_size = lin_start\n",
    "        lin_layers, last_feature_size = build_layers(\n",
    "            n_lin, self.block, previous_feature_size, current_feature_size\n",
    "        )\n",
    "        self.blocks = nn.Sequential(lin_layers)\n",
    "        self.age_gap = nn.Linear(last_feature_size, 3)\n",
    "        # self.activation = nn.Softmax()\n",
    "    \n",
    "    def block(self, in_features, out_features):\n",
    "        lin = nn.Linear(in_features, out_features)\n",
    "        relu = nn.ReLU(inplace=True)\n",
    "        return nn.Sequential(lin, relu)\n",
    "    \n",
    "    def get_z(self, x):\n",
    "        z = self.cnn_backbone(x)\n",
    "        z = torch.reshape(z, (z.shape[0], z.shape[1] * z.shape[2] * z.shape[3]))\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2 = x\n",
    "        z1 = self.get_z(x1)\n",
    "        z2 = self.get_z(x2)\n",
    "        z = torch.cat([z1, z2], axis=-1)\n",
    "        z = self.blocks(z)\n",
    "        z = self.age_gap(z)\n",
    "        # z = torch.reshape(z, (z.shape[0],))\n",
    "        # z = self.activation(z)\n",
    "        return z\n",
    "\n",
    "learning_rate = 0.001\n",
    "dual_learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 500\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = MandrillImageDataset(root_dir='Images', dataframe=one_year_data, in_mem=True)\n",
    "dual_dataset = MandrillDualImageDataset(root_dir='Images', dataframe=data, in_mem=True)\n",
    "\n",
    "def split_dataset(dataset, train_ratio, batch_size):\n",
    "    # Split the dataset into training and validation subsets\n",
    "    train_size = int(train_ratio * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader, train_dataset, val_dataset\n",
    "\n",
    "train_loader, val_loader, train_dataset, val_dataset = split_dataset(dataset, train_ratio, batch_size)\n",
    "dual_train_loader, dual_val_loader, _, _ = split_dataset(dual_dataset, train_ratio, batch_size)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "# Model\n",
    "backbone = BackboneRegressionModel()\n",
    "backbone = backbone.to(device)\n",
    "model = RegressionModel(backbone)\n",
    "dual_model = DualRegressionModel(backbone)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.MSELoss()\n",
    "dual_criterion = nn.CrossEntropyLoss()\n",
    "val_criterion = nn.L1Loss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "dual_optimizer = optim.Adam(model.parameters(), lr=dual_learning_rate)\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "dual_model = dual_model.to(device)\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "    \n",
    "    \n",
    "def train_step(loader, index, optimizer, model, criterion):\n",
    "    images, labels = next(iter(loader))\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    size = 0\n",
    "    if isinstance(images, list):\n",
    "        size = images[0].size(0)\n",
    "    else:\n",
    "        size = images.size(0)\n",
    "\n",
    "    return loss.item() * size\n",
    "    \n",
    "def save(backbone, model, dual_model, prefix):\n",
    "    torch.save(backbone.state_dict(), f\"models/{prefix}_exp3_backbone.h5\")\n",
    "    torch.save(model.state_dict(), f\"models/{prefix}_exp3_model.h5\")\n",
    "    torch.save(dual_model.state_dict(), f\"models/{prefix}exp3_dual_model.h5\")\n",
    "\n",
    "def load(backbone, model, dual_model, prefix):\n",
    "    backbone.load_state_dict(torch.load(f\"models/{prefix}_exp3_backbone.h5\"))\n",
    "    model.load_state_dict(torch.load(f\"models/{prefix}_exp3_model.h5\"))\n",
    "    dual_model.load_state_dict(torch.load(f\"models/{prefix}exp3_dual_model.h5\"))\n",
    "    return backbone, model, dual_model\n",
    "    \n",
    "#######\n",
    "train = False\n",
    "#######\n",
    "    \n",
    "if train:\n",
    "    # Training loop\n",
    "    best_val = np.inf\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to train mode\n",
    "        train_loss = 0.0\n",
    "        dual_train_loss = 0.0\n",
    "        steps = len(train_loader)\n",
    "\n",
    "        for i in tqdm(range(steps)):\n",
    "            train_loss += train_step(train_loader, i, optimizer, model, criterion)\n",
    "            dual_train_loss += train_step(dual_train_loader, i, dual_optimizer, dual_model, dual_criterion)\n",
    "\n",
    "        train_loss /= len(train_dataset)\n",
    "        dual_train_loss /= len(train_dataset)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader):\n",
    "                outputs = model(images)\n",
    "                loss = val_criterion(outputs, labels)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "\n",
    "        val_loss /= len(val_dataset)\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            print(f\"Val loss improved from {best_val:.4f} to {val_loss:.4f}\")\n",
    "            best_val = val_loss\n",
    "            save(backbone, model, dual_model, \"best\")\n",
    "        else:\n",
    "            print(f\"Val loss did not improved from {best_val:.4f}\")\n",
    "\n",
    "        # Print training and validation metrics\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
    "              f\"Train Loss: {train_loss:.5f} - \"\n",
    "              f\"Train Dual loss: {dual_train_loss:.5f} - \"\n",
    "              f\"Val L1: {val_loss:.5f}\")\n",
    "\n",
    "    torch.save(backbone.state_dict(), \"models/exp3_backbone.h5\")\n",
    "    torch.save(model.state_dict(), \"models/exp3_model.h5\")\n",
    "    torch.save(dual_model.state_dict(), \"models/exp3_dual_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "backbone, model, dual_model = load(backbone, model, dual_model, \"best\")\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "max_display = 10\n",
    "\n",
    "age_errors = []\n",
    "age_truth = []\n",
    "age_predicted = []\n",
    "\n",
    "# Perform inference on validation images\n",
    "for i, (images, targets) in enumerate(val_loader):\n",
    "    # Forward pass\n",
    "    outputs = model(images)\n",
    "    \n",
    "    # Convert the outputs to numpy arrays\n",
    "    predicted_values = outputs.squeeze().detach().cpu().numpy() * 365\n",
    "    actual_values = targets.squeeze().cpu().numpy() * 365\n",
    "    \n",
    "    age_errors.append(predicted_values - actual_values)\n",
    "    age_truth.append(actual_values)\n",
    "    age_predicted.append(predicted_values)\n",
    "    \n",
    "    if i >= max_display:\n",
    "        continue\n",
    "    \n",
    "    # Display the results\n",
    "    print(\"Predicted Values:\", predicted_values)\n",
    "    print(\"Actual Values:\", actual_values)\n",
    "    print(\"Prediction Error: \", predicted_values - actual_values)\n",
    "    print()  # Add an empty line for separation\n",
    "    \n",
    "    # Visualize the images and predictions\n",
    "    plt.imshow(images.squeeze().cpu().permute(1, 2, 0))\n",
    "    plt.title(f\"Predicted: {predicted_values}, Actual: {actual_values}\")\n",
    "    plt.show()\n",
    "\n",
    "age_errors = np.array(age_errors)\n",
    "print(\"Absolute age error (in days): \", np.mean(age_errors), \" std: \", np.std(age_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram with x being the real age and y being the error on this age\n",
    "# Plotting the histogram\n",
    "\n",
    "def group_by(x, n_bins):\n",
    "    step = np.max(x) / n_bins\n",
    "    x = np.array(x)\n",
    "    for i in range(n_bins+1):\n",
    "        nx = i*step\n",
    "        px = max(0, (i-1)*step)\n",
    "        x[np.logical_and(x <= nx, x > px)] = nx\n",
    "    x[np.logical_and(x <= np.max(x), x > (n_bins*step))] = np.max(x)\n",
    "    return x.tolist()\n",
    "\n",
    "def bin_errors(x, y):\n",
    "    bins = np.unique(x)\n",
    "    y_mean = []\n",
    "    y_err = []\n",
    "    y = np.array(y)\n",
    "    for b in bins:\n",
    "        y_bin = y[x == b]\n",
    "        y_mean.append(np.mean(y_bin))\n",
    "        y_err.append(np.std(y_bin))\n",
    "    return np.array(y_mean), np.array(y_err)\n",
    "\n",
    "n_bins = 6\n",
    "\n",
    "x = age_truth\n",
    "y = age_errors\n",
    "\n",
    "sorted_lists = sorted(zip(x, y))\n",
    "sorted_x, sorted_y = zip(*sorted_lists)\n",
    "\n",
    "def display_error_curve(x, y, n_bins):\n",
    "    x = group_by(x, n_bins)\n",
    "    y, y_err = bin_errors(x, y)\n",
    "    x = np.unique(x)\n",
    "\n",
    "    plt.xscale(\"log\")\n",
    "    plt.plot(x, y, 'k-', label='Prediction error')\n",
    "    plt.fill_between(x, y-y_err, y+y_err)\n",
    "    plt.show()\n",
    "\n",
    "display_error_curve(sorted_x, sorted_y, n_bins)\n",
    "display_error_curve(sorted_x, abs(np.array(sorted_y)), n_bins)\n",
    "# Display the mean value per bin\n",
    "#for i in range(n_bins):\n",
    "#    plt.text(x_bins[i], max(x_hist[i], y_hist[i]), f'{x_mean:.2f}', ha='center', va='bottom')\n",
    "#    plt.text(y_bins[i], max(x_hist[i], y_hist[i]), f'{y_mean:.2f}', ha='center', va='bottom')\n",
    "\n",
    "\n",
    "# Distribution of image according to the age\n",
    "hist = one_year_data[\"age\"].hist(bins=n_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vtf",
   "language": "python",
   "name": "vtf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
